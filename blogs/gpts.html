<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Murat Onur Yildirim - GPTs</title>
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="./blog-styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <div class="container">

    <aside>
      <img src="../imgs/profile.webp" alt="Murat Onur Yildirim's Profile Picture">
      <h3>Murat Onur Yildirim</h3>
      <p class="description">PhD Candidate in AI</p>
      <ul class="social-links">
        <li><a href="../index.html">Home</a></li>
        <li><a href="../index.html#publications">Publications</a></li>
        <li><a href="../index.html#blog-posts" class="active">Blogs</a></li>
        <li> - </li>
        <li><a href="https://linkedin.com/in/muratonuryildirim" target="_blank">LinkedIn</a></li>
        <li><a href="https://github.com/muratonuryildirim" target="_blank">GitHub</a></li>
        <li><a href="https://scholar.google.com/citations?user=3NAjgx0AAAAJ&hl=en" target="_blank">Scholar</a></li>
        <li><a href="https://twitter.com/monuryildirim" target="_blank">X</a></li>
      </ul>
    </aside>

    <main id="main">
      <header>
        <h1>A Deep Dive into AI Marvel: Generative Pretrained Transformers (GPTs)</h1>
      </header>
        
      <section>
        <ol>
          <li>
            In recent years, the field of artificial intelligence (AI) has witnessed remarkable advancements, particularly in natural language processing (NLP). 
            Among the most significant breakthroughs are the Generative Pretrained Transformers (GPTs), a series of language models developed by OpenAI. 
            GPTs have transformed the way we interact with machines, offering unprecedented capabilities in understanding and generating human language. 
            This blog post delves into the evolution, architecture, training, inference, and impact of GPTs, highlighting their transformative potential across various industries.
          </li>
        </ol>
        
        
        <h2> The Genesis of GPTs </h2>
        <ol>
          <li> GPTs are based on the Transformer architecture, introduced by Vaswani et al. in 2017. 
            The Transformer model revolutionized NLP by employing self-attention mechanisms, 
            which enable the model to weigh the importance of different words in a sentence, regardless of their position. 
            This was a departure from the traditional recurrent neural networks (RNNs) and long short-term memory (LSTM) networks, which struggled with long-range dependencies.
          </li>
        </ol>

        <figure>
          <img src="../imgs/blogs/autoregression.png" alt="Description of the image" class="center" style="width: 55rem;">
        </figure>

        <h2> How GPTs Work </h2>
        <ol>
          <li>
          <b>Pretraining:</b> The model is trained on a vast corpus of text in an unsupervised manner, learning to predict the next word in a sentence. This helps the model capture general language patterns and structures.
          During pretraining, the objective is to minimize the cross-entropy loss between the predicted word probabilities and the actual words in the training data. The loss function can be defined as:
          
          \[ L = -\sum_{i=1} \log P(w_i | w_1, w_2, ..., w_{i-1}) \]

          where \( w_i \) is the actual word at position \( i \), and \( P(w_i | w_1, w_2, ..., w_{i-1}) \) is the predicted probability of the word given the preceding words.
          The optimization is typically performed using stochastic gradient descent (SGD) or its variants, such as Adam, which adjust the model parameters to minimize the loss.
          </li>
          <li>
          <b>Finetuning or Prompting:</b> Initial GPTs further trained on specific tasks with labeled data. This step tailors the model to perform particular tasks, such as translation, summarization, or question answering.
          Next generation GPTs, like GPT-3, have demonstrated the ability to perform tasks without finetuning, relying on prompts or examples to guide the generation process referred as <i>in-context learning</i>.
          </li>
          <li>
          <b>During inference,</b> the model generates text by predicting the next word in a sequence, given the preceding words. 
          The process is autoregressive, meaning the model generates one word at a time and then uses that word as part of the context for generating the next word. 
          The model samples from the probability distribution \( P(w_i | w_1, w_2, ..., w_{i-1}) \) to generate coherent and contextually relevant text. 
          This can be done using methods like;
          </li>
          <li>
            <i>Greedy Search:</i> Selecting the word with the highest probability at each step.
          </li>
          <li>
            <i>Beam Search:</i> Exploring multiple sequences and selecting the most probable one.
          </li>
          <li>
            <i>Sampling:</i> Randomly selecting words based on their probabilities, which can produce more diverse and creative outputs.
          </li>
          <li>
            <i>Top-k Sampling:</i> Restricting sampling to the top k most probable words.
          </li>
          <li>
            <i>Nucleus Sampling (Top-p):</i> Sampling from the smallest set of words whose cumulative probability exceeds a threshold p.
          </li>
        </ol>
        
        <figure>
          <img src="../imgs/blogs/gpt1.png" alt="Description of the image" class="center" style="width: 40rem;">
        </figure>

        <h2>Evolution of GPT Models</h2>
        <h3>GPT-1: The First Step</h3>
        <ol>
          <li><i>Released: 2018, Parameters: 110 million.</i></li>
          <li>
          GPT-1 was the first model in the series, introducing the concept of using a large corpus of text for pretraining followed by finetuning on specific tasks.
          The model demonstrated the potential of this approach, achieving state-of-the-art performance on several NLP benchmarks. 
          However, it had limitations in handling long-term dependencies and generating coherent long-form text.
          </li>
          <li>
            <b>Transformer Decoder:</b> Utilizes only the decoder part of the Transformer architecture.
          </li>
          <li>
            <b>Two-step training process:</b> It is consist of pretraining and finetuning steps which are the basis for following GPTs.
          </li>
        </ol>

        <h3>GPT-2: Scaling Up</h3>
        <ol>
          <li><i>Released: 2019, Parameters: 1.5 billion.</i></li>
          <li>GPT-2 represented a significant leap in scale and performance. It is designed to show that scaling up the model size can lead to better results even without finetuning step (zero-shot).
            With 1.5 billion parameters, it demonstrated an impressive ability to generate coherent and contextually relevant text. 
          </li>
          <li> <b>Larger Scale:</b>Ten times more parameters than GPT-1, leading to improved performance even without finetunung.</li>
          <li> <b>Zero-Shot Learning:</b> Demonstrated the ability to perform tasks it wasn’t explicitly trained on by leveraging its general language understanding.</li>
        </ol>

        <h3>GPT-3: The Giant Leap</h3>
        <ol>
          <li><i>Released: 2020, Parameters: 175 billion.</i></li>
          <li>GPT-3 is one of the largest and most powerful model in the series to date, with 175 billion parameters. 
            Its vast scale allows it to generate human-like text with remarkable coherence and context awareness. 
            GPT-3’s capabilities extend far beyond its predecessors, making it useful in a wide array of applications, from creative writing to complex problem-solving.
          </li>
          <li><b>Prompt-Based Learning:</b> To remove the finetuning for each specific task, unlike its predecessors, GPT-3 primarily uses promps to perform tasks. 
            Users provide examples or instructions in the prompt, and the model generates appropriate responses based on that context instead of finetuning.
            The model is large enough that providing prompts functions similarly to finetuning, enabling it to perform the required or requested tasks.</li>
          <li><b>One-Shot and Few-Shot Learning:</b> Prompts act as one-shot or few-shot learning that can perform specific tasks with minimal examples, reducing the need for extensive finetuning.</li>
          <li><b>Broad Applications:</b> GPT-3 powers applications like chatbots, content generation tools, and even programming aids like GitHub Copilot.</li>
          <li><b>Resource Intensity:</b> Training GPT-3 requires substantial computational resources and energy, raising accessibility and environmental concerns.</li>
          <li><b>Bias and Fairness:</b> With more parameters, the model can also amplify biases present in the training data, necessitating advanced mitigation strategies.</li>
        </ol>

        
        <figure>
          <img src="../imgs/blogs/incontext.png" alt="Description of the image" class="center" style="width: 60rem;">
        </figure>

        <h2>Ethical Considerations and Challenges</h2>
        <ol>
          <li>
            <b>Bias and Fairness:</b> GPTs can perpetuate and amplify biases present in the training data, leading to unfair or harmful outcomes. Addressing this requires ongoing research and the development of mitigation strategies.
          </li>
          <li>
            <b>Misinformation and Misuse:</b> The ability of GPTs to generate convincing text can be exploited to spread misinformation or create deepfake content. Ensuring responsible use is crucial.
          </li>
          <li>
            <b>Resource Intensity:</b> Training large models like GPT-3 requires significant computational resources and energy, raising concerns about the environmental impact and accessibility.
          </li>
          <li>
            <b>Interpretability:</b> Understanding the decision-making process of GPTs remains challenging, hindering the ability to fully trust and control AI outputs.
          </li>
          <li>
            <b>Privacy Concerns:</b> Ensuring that sensitive information in training data is not inadvertently disclosed through the model's outputs.
          </li>
        </ol>
        
        <h2>The Future of GPTs</h2>
        <ol>
          <li><b>Hybrid Models:</b> Combining GPTs with other AI models to leverage the strengths of each.</li>
          <li><b>Interactive AI:</b> Developing models that can interact with other AI systems and humans in more dynamic ways.</li>
          <li><b>Sustainability:</b> Focusing on reducing the environmental impact of training large AI models.</li>
        </ol>
        
        <h2>Wrapping up</h2>
        <ol>
          <li>
            GPTs represent a monumental achievement in the field of AI, pushing the boundaries of what machines can understand and create in human language. 
            Their impact spans various domains, from creative writing to programming, showcasing the transformative potential of advanced language models. 
            As we continue to develop and refine these technologies, it is imperative to address the associated ethical challenges, 
            ensuring that GPTs contribute positively to the advancement of human knowledge and society.
          </li>
        </ol>
      
      </section>

          <!-- Add more content as needed -->

      </main>

  </div>

</body>
</html>
