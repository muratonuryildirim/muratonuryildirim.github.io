<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Murat Onur Yildirim - Diffusion Models</title>
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="./blog-styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <div class="container">

    <main id="main">
      <header>
        <h1>Unleashing Creativity with Diffusion Models</h1>
      </header>
        
      <section>
        <ol>
          <li>
            Imagine an AI that can turn a block of random noise into a breathtaking image, or better yet, generate an image based on your wildest textual descriptions! 
            Sounds like science fiction, right? Welcome to the fascinating world of diffusion models—a cutting-edge technology in deep learning that's pushing the boundaries of generative models. 
            Let's dive deep into how these models work, how they're trained, and how they can generate incredible images from textual prompts.
          </li>
        </ol>

        <figure>
          <img src="../imgs/blogs/diffusion0.png" alt="Description of the image" class="center" style="width: 40rem;">
        </figure>

        <h2>What Are Diffusion Models?</h2>
        <ol>
          <li>Diffusion models are a type of generative model that creates data by starting with pure noise and gradually refining it until it looks like real data. 
            Think of it as sculpting a statue: you start with a rough block (noise) and chisel away (refine) until you reveal the masterpiece (image). It has two-phase process:
          </li>
          <li><b>Forward Process (Diffusion):</b> This phase involves adding noise to an image step by step until it becomes pure noise. 
            Imagine gradually blurring a photo until it's completely unrecognizable.
            Mathematically, we add Gaussian noise at each step, controlled by a noise schedule function \( β_t \).
          </li>
          <li><b>Reverse Process (Denoising):</b> Here, the model learns to reverse the noise addition. 
            It starts with a noisy image and iteratively removes the noise, trying to recover the original image.
          </li>
        </ol>

        <h3>Training a Diffusion Model: The Nitty-Gritty</h3>
        <ol>
          <li>Training a diffusion model involves teaching it to predict and remove noise from images at various stages of the forward process. 
            Here's a step-by-step breakdown of the training process.</li>
          <li><b>Data Preparation:</b> Start with a dataset of real images \({x_0}\). These images are your training ground.</li>
          <li><b>Adding Noise (Forward Process):</b> For each image \( x_0 \), select a random time step \( t \) 
            (ranging from 1 to total number of steps \( T \)) and add noise to create a sequence of noisy images \( x_t \).
          </li>
          <li>\[x_t = \sqrt{\alpha_t} x_0 + \sqrt{1 - \alpha_t} * ε\]</li>
          <li>Here, \(ε\) is Gaussian noise, and \(\alpha_t\) is a cumulative product of noise factors \(β_t\).</li>
          <li><b>Training Objective:</b> The goal is to train a neural network \(ε_θ\) to predict the added noise \(ε\) by learning 
            to minimize the difference between the true noise and the predicted noise.
          </li>
          <li>\[ \mathcal{L} = E_{x_0, t, ε} [ || ε - ε_θ(x_t, t) ||^2 ] \]</li>
        </ol>
        
        <figure>
          <img src="../imgs/blogs/glide.png" alt="Description of the image" class="center" style="width: 40rem;">
        </figure>

        <h3>Training Text Guided Diffusion Model: Integrating Text Prompts</h3>
        <ol>
          <li>Now, let's sprinkle some magic dust on our diffusion model and make it text-guided. Imagine describing an image in words and having the AI bring it to life!</li>
          <li><b>Text-Image Pairs:</b> Use a dataset with text-image pairs (e.g., MS COCO).</li>
          <li><b>Text Embedding:</b> Convert textual descriptions into embeddings using a pre-trained text encoder like CLIP.</li>
          <li><b>Conditional Diffusion Model:</b> Modify the diffusion model to condition on both the noisy image \(x_t\) and the text embedding \(z\).</li>
          <li><b>Sample a Text-Image Pair:</b> Take an image \(x_0\) and its corresponding text description.</li>
          <li><b>Generate Text Embedding:</b> Convert the text description to a text embedding \(z\) using CLIP.</li>
          <li><b>Add Noise to Image:</b> For a randomly selected time step \(t\), add noise to the image \(x_0\).</li>
          <li><b>Train the Model to Predict Noise:</b> Predict the noise added to \(x_t\), conditioned on both \(x_t\) and the text embedding \(z\).</li>
          <li>\[ \mathcal{L} = E_{x_0, t, ε, z} [ || ε - ε_θ(x_t, t, z) ||^2 ] \]</li>
        </ol>

        <h3>Generating New Images: The Artistic Touch</h3>
        <ol>
          <li>
            Once trained, the model can generate new images by starting with pure noise and gradually refining it.
          </li>
          <li><b>Start with Noise:</b> Begin with a random noise image \(x_T\).</li>
          <li><b>Iterative Denoising:</b> For each time step \(t\) from \(T\) down to 1, use the trained model to predict the noise \(ε_t\).
            Then, remove the predicted noise to get the previous step's image \(x_{t-1}\) conditioned on both \(x_t\) (and the text embedding \(z\)).</li>
          <li>\[x_{t-1} = \frac{1}{\sqrt{\alpha_t}} (x_t - \frac{β_t}{\sqrt{1 - \alpha_t}}) ε_θ(x_t, t)\]</li>
        </ol>


        <figure>
          <img src="../imgs/blogs/diffusion-algo.png" alt="Description of the image" class="center" style="width: 40rem;">
        </figure>

        <h2>Applications of Diffusion Models</h2>
        <ol>
          <li><b>Image inpainting:</b> Diffusion models can be used to fill in missing or corrupted parts of an image, effectively "inpainting" the missing information.</li>
          <li><b>Style transfer:</b> By conditioning the diffusion model on a style image, it can generate new images that combine the content of one image with the style of another.</li>
          <li><b>Data augmentation:</b> Diffusion models can be used to generate synthetic data for training machine learning models, helping to increase the size and diversity of the training dataset.</li>
          <li><b>Video synthesis:</b> By extending the diffusion process to video frames, diffusion models can generate realistic and coherent video sequences.</li>
          <li><b>Text-to-image synthesis:</b> By conditioning the diffusion model on a text prompt, it can generate images that match the textual description, enabling text-to-image synthesis.</li>
        </ol>

        <h2>Wrapping Up</h2>
        <ol>
          <li>
            Diffusion models represent a giant leap forward in the field of generative models, capable of producing stunningly realistic images. 
            By learning to denoise progressively, these models can transform random noise into intricate images. 
            When coupled with text prompts, they unlock a whole new level of creativity, turning your words into visual art. 
            Whether you're a researcher, artist, or just an AI enthusiast, diffusion models offer a glimpse into the future of AI-generated content. 
            So, what image will you create today?
          </li>
        </ol>

      </section>

          <!-- Add more content as needed -->

      </main>

  </div>

</body>
</html>
